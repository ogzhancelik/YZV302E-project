{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d952e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2283b",
   "metadata": {},
   "source": [
    "Article URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09207426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration --\n",
    "base_url = \"https://tr.khanacademy.org\"\n",
    "start_url = \"https://tr.khanacademy.org/science/biology\"\n",
    "output_filename = \"data/khanacademy_bio_articles_urls.json\"\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"--log-level=3\") \n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "data = {}\n",
    "\n",
    "article_pattern = re.compile(r\"/a/\")\n",
    "\n",
    "try:\n",
    "    for class_num in range(1, 2):    \n",
    "        print(f\"Accessing Main Page: {start_url}\")\n",
    "        driver.get(start_url)\n",
    "        \n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        time.sleep(random.randint(2, 4))\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        #Find all Unit URLs\n",
    "        unit_links = []\n",
    "        raw_links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for a in raw_links:\n",
    "            href = a['href']\n",
    "            if \"/science/biology/\" in href and href != \"/science/biology\" and \"javascript\" not in href:\n",
    "                full_unit_url = base_url + href if href.startswith(\"/\") else href\n",
    "                if full_unit_url not in unit_links:\n",
    "                    unit_links.append(full_unit_url)\n",
    "\n",
    "        # Initialize list for this class\n",
    "        class_data = []\n",
    "\n",
    "        # Process each Unit\n",
    "        for unit_number, unit_url in enumerate(tqdm(unit_links, desc=f\"Scraping Class {class_num}\"), start=1):\n",
    "            driver.get(unit_url)\n",
    "            time.sleep(random.randint(1, 3)) # Sleep to be polite and let JS load\n",
    "            \n",
    "            unit_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find Article URLs in this Unit\n",
    "            found_urls = []\n",
    "            \n",
    "            # Find all links on the page\n",
    "            all_anchors = unit_soup.find_all(\"a\", href=True)\n",
    "            \n",
    "            for anchor in all_anchors:\n",
    "                href = anchor['href']\n",
    "                # Check if it is an article link (/a/) and not a video link (/v/)\n",
    "                if \"/a/\" in href:\n",
    "                    full_url = base_url + href if href.startswith(\"/\") else href\n",
    "                    \n",
    "                    # Avoid duplicates and ensure it's a valid link\n",
    "                    if full_url not in found_urls:\n",
    "                        found_urls.append(full_url)\n",
    "            \n",
    "            if not found_urls:\n",
    "                class_data.append({\n",
    "                    \"unit_number\": unit_number,\n",
    "                    \"non_video_urls\": None\n",
    "                })\n",
    "            else:\n",
    "                class_data.append({\n",
    "                    \"unit_number\": unit_number,\n",
    "                    \"non_video_urls\": found_urls\n",
    "                })\n",
    "\n",
    "        data[f\"class_{class_num}\"] = class_data\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# Save to JSON\n",
    "with open(f\"./data/{output_filename}\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data has been successfully saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606df6a",
   "metadata": {},
   "source": [
    "Video URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d252dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "base_url = \"https://tr.khanacademy.org\"\n",
    "start_url = \"https://tr.khanacademy.org/science/biology\"\n",
    "output_filename = \"data/khanacademy_bio_vids_urls.json\"\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"--log-level=3\") \n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "data = {}\n",
    "\n",
    "try:\n",
    "    for class_num in range(1, 2):    \n",
    "        print(f\"Accessing Main Page: {start_url}\")\n",
    "        driver.get(start_url)\n",
    "\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        time.sleep(random.randint(2, 4))\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find all Unit URLs\n",
    "        unit_links = []\n",
    "        raw_links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for a in raw_links:\n",
    "            href = a['href']\n",
    "            if \"/science/biology/\" in href and href != \"/science/biology\" and \"javascript\" not in href:\n",
    "                full_unit_url = base_url + href if href.startswith(\"/\") else href\n",
    "                if full_unit_url not in unit_links:\n",
    "                    unit_links.append(full_unit_url)\n",
    "\n",
    "        # Initialize list for this class\n",
    "        class_data = []\n",
    "\n",
    "        print(f\"Found {len(unit_links)} units.\")\n",
    "\n",
    "        # Process each Unit\n",
    "        for unit_number, unit_url in enumerate(tqdm(unit_links, desc=f\"Scraping Class {class_num}\"), start=1):\n",
    "            driver.get(unit_url)\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            except:\n",
    "                print(f\"Timeout loading {unit_url}\")\n",
    "                \n",
    "            time.sleep(random.randint(1, 3)) \n",
    "\n",
    "            unit_soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find Video URLs in this Unit\n",
    "            unit_video_links = []\n",
    "            \n",
    "            all_anchors = unit_soup.find_all(\"a\", href=True)\n",
    "            \n",
    "            for anchor in all_anchors:\n",
    "                href = anchor['href']\n",
    "                \n",
    "                # Check for video link pattern /v/\n",
    "                if \"/v/\" in href:\n",
    "                    full_url = base_url + href if href.startswith(\"/\") else href\n",
    "                    \n",
    "                    # Avoid duplicates\n",
    "                    if full_url not in unit_video_links:\n",
    "                        unit_video_links.append(full_url)\n",
    "            \n",
    "            if not unit_video_links:\n",
    "                class_data.append({\n",
    "                    \"unit_number\": unit_number,\n",
    "                    \"video_number\": 0,\n",
    "                    \"video_urls\": None\n",
    "                })\n",
    "            else:\n",
    "                class_data.append({\n",
    "                    \"unit_number\": unit_number,\n",
    "                    \"video_number\": len(unit_video_links),\n",
    "                    \"video_urls\": unit_video_links\n",
    "                })\n",
    "\n",
    "        data[f\"class_{class_num}\"] = class_data\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open(output_filename, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data has been successfully saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e838df6",
   "metadata": {},
   "source": [
    "Article Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e693f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = re.sub(r'[^\\S\\r\\n]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"Created with Raphaël\", \"\").strip()\n",
    "    return text\n",
    "\n",
    "def remove_duplicates_preserve_order(input_list):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for item in input_list:\n",
    "        cont = False\n",
    "        for result_item in result:\n",
    "            if item in result_item:\n",
    "                cont = True\n",
    "                break\n",
    "        if cont:\n",
    "            continue\n",
    "        if item not in seen:\n",
    "            result.append(item)\n",
    "            seen.add(item)\n",
    "    return result\n",
    "\n",
    "def add_space_after_punctuation(text):\n",
    "    pattern = r'([.,!?;:])(?=\\S)'\n",
    "    corrected_text = re.sub(pattern, r'\\1 ', text)\n",
    "    return corrected_text\n",
    "\n",
    "def split_camel_case(text):\n",
    "    split_text = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', text)\n",
    "    return split_text\n",
    "\n",
    "def read_json(file_path):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Initialize tqdm progress bar with a fixed total of 126 URLs\n",
    "    with tqdm(total=126, desc=\"Processing URLs\") as pbar:\n",
    "        for class_name, units in data.items():\n",
    "            for unit in units:\n",
    "                unit_number = unit[\"unit_number\"]\n",
    "                non_video_urls = unit.get(\"non_video_urls\")\n",
    "\n",
    "                if non_video_urls:\n",
    "                    for url in non_video_urls:\n",
    "                        try:\n",
    "                            driver.get(url)\n",
    "                            WebDriverWait(driver, 10).until(\n",
    "                                EC.presence_of_element_located((By.CLASS_NAME, '_1ge0o9y3'))\n",
    "                            )\n",
    "                            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                            titles = soup.find_all('h1', class_='_1ge0o9y3')\n",
    "                            print(\"title\", titles)\n",
    "                            title_text = \"\"\n",
    "                            for title in titles:\n",
    "                                title_text = title.get_text(strip=True)\n",
    "\n",
    "                            # Wait until the parent element with class '_1h1mqh3' is present\n",
    "                            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, '_1h1mqh3')))\n",
    "\n",
    "                            parent_element = soup.find('div', class_='_1h1mqh3')\n",
    "\n",
    "                            content_list = []\n",
    "\n",
    "                            if parent_element:\n",
    "                                sub_elements = parent_element.find_all(class_='paragraph')\n",
    "                                \n",
    "                                for sub_element in sub_elements:\n",
    "                                    text = clean_text(sub_element.get_text())\n",
    "                                    if text and text not in content_list:\n",
    "                                        content_list.append(text)\n",
    "\n",
    "                            content_list = remove_duplicates_preserve_order(content_list)\n",
    "                            full_contents = \"\\n\".join(content_list)\n",
    "                            full_contents = add_space_after_punctuation(full_contents)\n",
    "                            full_contents = split_camel_case(full_contents)\n",
    "\n",
    "                            results.append({\n",
    "                                \"url\": url,\n",
    "                                \"title\": title_text,\n",
    "                                \"contents\": full_contents\n",
    "                            })\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {url}: {e}\")\n",
    "                            # Continue to the next URL if an error occurs\n",
    "                            continue\n",
    "\n",
    "                        # Update progress bar\n",
    "                        pbar.update(1)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    fname = \"data/biology_extracted_articles.json\"\n",
    "\n",
    "    # Write results to a new JSON file\n",
    "    with open(fname, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(results, outfile, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Contents successfully saved to {fname}\")\n",
    "\n",
    "file_path = 'data/khanacademy_bio_articles_urls.json'\n",
    "read_json(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e1464",
   "metadata": {},
   "source": [
    "Video Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfba112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_link_number(file_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    total = 0\n",
    "    # Initialize tqdm progress bar with a fixed total of 126 URLs\n",
    "    for _ , units in data.items():\n",
    "\n",
    "        for unit in units:\n",
    "            \n",
    "            video_number = unit.get(\"video_number\")\n",
    "\n",
    "            total += video_number\n",
    "            \n",
    "    return total\n",
    "\n",
    "def read_json(file_path):\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "\n",
    "    # Initialize tqdm progress bar with a fixed total of 126 URLs\n",
    "    with tqdm(total=get_total_link_number(file_path), desc=\"Processing URLs\") as pbar:\n",
    "        for class_num , units in data.items():\n",
    "\n",
    "            class_unit_data = []\n",
    "            for unit in units:\n",
    "                \n",
    "                video_urls = unit.get(\"video_urls\")\n",
    "                unit_number = unit.get(\"unit_number\")\n",
    "\n",
    "                if video_urls:\n",
    "                    for url in video_urls:\n",
    "                        try:\n",
    "                            driver.get(url)\n",
    "                            time.sleep(random.randint(2, 3))\n",
    "\n",
    "                            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                            # Wait until the titles with class '_qts7xbw' are present\n",
    "                            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, '_1ge0o9y3')))\n",
    "\n",
    "                            titles = soup.find_all('h1', class_='_1ge0o9y3')\n",
    "                            title_text = \"\"\n",
    "                            for title in titles:\n",
    "                                title_text = title.get_text(strip=True)\n",
    "\n",
    "                            transcript_text_content = None\n",
    "                            try:\n",
    "                                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, '_1fezbb8')))\n",
    "                                transcript_parent = soup.find(class_=\"_1fezbb8\")\n",
    "                                transcript_text_content = transcript_parent.get_text()\n",
    "                            except TimeoutException:\n",
    "                                print(\"Element with class '_1fezbb8' not found within the given time.\")\n",
    "\n",
    "                            class_unit_data.append({\n",
    "                                \"url\": url,\n",
    "                                \"title\": title_text,\n",
    "                                \"transcript_content\": transcript_text_content\n",
    "                            })\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing {url}: {e}\")\n",
    "                            # Continue to the next URL if an error occurs\n",
    "                            continue\n",
    "\n",
    "                        # Update progress bar\n",
    "                        pbar.update(1)\n",
    "\n",
    "            results.append({\n",
    "                \"class_num\": class_num,\n",
    "                \"contents\": class_unit_data\n",
    "            })\n",
    "            \n",
    "    driver.quit()\n",
    "\n",
    "    fname = \"data/biology_video_transcripts.json\"\n",
    "\n",
    "    # Write results to a new JSON file\n",
    "    with open(fname, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(results, outfile, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Contents successfully saved to {fname}\")\n",
    "\n",
    "file_path = 'data/khanacademy_bio_vids_urls.json'\n",
    "read_json(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc5f0f",
   "metadata": {},
   "source": [
    "Generating Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab156b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b86b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_prompt(text, title):\n",
    "    prompt = f\"\"\"\n",
    "    Biyoloji dersinde, verilen metnin başlığı '{title}'. Bu metin aslında bir video transkripti; \n",
    "    bu yüzden metni dikkatlice okuyup, konuya dair anlamlı, hatasız ve tekrarsız 4 soru üretmeniz bekleniyor.\n",
    "    \n",
    "    - Sorular yalnızca metinde açıkça belirtilen bilgilere dayanmalıdır.\n",
    "    - Videoda gösterilen ancak metinde açıkça belirtilmeyen görsel veya figürlere dayalı sorulardan kaçının.\n",
    "    - Eğer transkript sadece belirli bir örneği açıklıyorsa, o örneğe bağlı kalmadan genel bilgiler üzerine sorular üretin.\n",
    "    - Soruların genel ve kavramsal bilgilere dayalı olması gerekmektedir; özel veya aşırı detaylı bilgilere dayanan sorulardan kaçınmalısınız.\n",
    "\n",
    "    Örneğin, metinde: \"Cebirsel ifadelerde ilk olarak eksi yedi ile sekiz çarpı x'in toplamı isteniyor. \n",
    "    Toplam dediği için burada toplama işlemi yapacağız.\"\n",
    "    İyi soru örneği: \"Cebirsel ifadelerde toplama işlemi nasıl yapılır?\"\n",
    "    Kötü soru örneği: \"Cebirsel ifadelerde toplama işleminin nasıl yapıldığını anlatırken hangi örneği kullanılmıştır?\" \n",
    "    Bu tür spesifik sorulardan kaçınmalısınız\n",
    "\n",
    "    Metin: {text}\n",
    "    \n",
    "    - Sorularınızı aşağıdaki formatta oluşturun:\n",
    "      İlk soru || İkinci soru || Üçüncü soru || Dördüncü soru\n",
    "    \n",
    "    Soruların kesinlikle belirtilen formatta olması gerekmektedir ve sorular dışında başka hiçbir şey yazılmamalıdır.\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8fe0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_article_prompt(text, title, course):\n",
    "    prompt = f\"\"\"\n",
    "    {course} dersinde, verilen metnin başlığı '{title}'.\n",
    "    Bu metni anlayıp, konuya dair anlamlı, doğru ve hatasız 10 soru üretmen bekleniyor.\n",
    "    Metin: {text}\n",
    "    \n",
    "    - Soruları aşağıdaki formatta oluştur:\n",
    "    üreteceğin ilk soru || üreteceğin ikinci soru || ...\n",
    "    \n",
    "    -Soruların kesinlikle belirtilen formatta olması gerekmektedir. soru hariç başka hiçbir şey yazmaman gerekmektedir,\n",
    "    \"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "773bf84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(text):\n",
    "    pattern = r'\\s*([^|]+?)\\s*(?=\\s*\\|\\|)'\n",
    "\n",
    "    questions = re.findall(pattern, text)\n",
    "    questions = [q.strip() for q in questions if q.strip()]\n",
    "    \n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d1abc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quest_dict(data, prompt_function):\n",
    "    answers = {}\n",
    "    not_formatted = []\n",
    "    client = connect_azure_gpt()\n",
    "    for example in data:\n",
    "        my_prompt = prompt_function(example['transcript_content'] ,example['title'] )\n",
    "        answer = generate_answer(client, my_prompt , \"\")\n",
    "        questions = get_questions(answer)\n",
    "        if questions:\n",
    "            answers[example['title']]= questions\n",
    "        else:\n",
    "            not_formatted.append(answer)\n",
    "    return answers, not_formatted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe6acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"data/biology_video_transcripts.json\"\n",
    "with open(file_name, encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    data = data[0]['contents']\n",
    "\n",
    "video_questions, _ = get_quest_dict(data, create_video_prompt)\n",
    "\n",
    "file_name = \"data/biology_extracted_articles.json\"\n",
    "with open(file_name, encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "article_questions, _ = get_quest_dict(data, create_article_prompt)\n",
    "\n",
    "all_questions = video_questions + article_questions\n",
    "\n",
    "output_file = 'biology_questions.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(all_questions, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
